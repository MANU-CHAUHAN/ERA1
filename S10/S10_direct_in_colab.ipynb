{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "aMIimxGGLcwa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ERA1"
      ],
      "metadata": {
        "id": "LxPKBAVSogic"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s18qOrYNmklj",
        "outputId": "da928e9b-312b-4ac3-bb8d-2f9629b43c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ERA1'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 128 (delta 66), reused 91 (delta 32), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (128/128), 13.06 MiB | 19.25 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/MANU-CHAUHAN/ERA1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRyVWVWtmxTZ",
        "outputId": "811f4959-c11e-4233-92f8-fb66f244794d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mERA1\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ERA1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jaj2bCWRm08y",
        "outputId": "9872afbf-f01e-4795-894a-bb608abc3c4f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ERA1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmklCZ6um3gE",
        "outputId": "8cb80e0d-ca31-4eca-e185-7a17bba89356"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main.py    README.md         \u001b[0m\u001b[01;34mresources\u001b[0m/  \u001b[01;34mS7\u001b[0m/  test.py   webapp.py\n",
            "models.py  requirements.txt  \u001b[01;34mS10\u001b[0m/        \u001b[01;34mS9\u001b[0m/  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W_yjPzCnGp0",
        "outputId": "4e4f83d5-c5ce-4e5e-a2b5-47a51f4eb312"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-22 01:39:36.059916: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-22 01:39:37.089705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "All requirements installed successfully.\n",
            "usage: main.py\n",
            "       [-h]\n",
            "       [--lr LR]\n",
            "       [--dataset DATASET]\n",
            "       [--model MODEL]\n",
            "       [--epochs EPOCHS]\n",
            "       [--lr_scheduler LR_SCHEDULER]\n",
            "       [--gamma GAMMA]\n",
            "       [--step_size STEP_SIZE]\n",
            "       [--optim OPTIM]\n",
            "       [--save SAVE]\n",
            "       [--max_lr MAX_LR]\n",
            "       [--start_lr START_LR]\n",
            "       [--batch BATCH]\n",
            "       [--pct_start PCT_START]\n",
            "       [--cutout_prob CUTOUT_PROB]\n",
            "       [--anneal_fn ANNEAL_FN]\n",
            "       [--cri CRI]\n",
            "       [--find_lr FIND_LR]\n",
            "       [--find_lr_iter FIND_LR_ITER]\n",
            "       [--Help]\n",
            "\n",
            "Training Deep Learning program for various models with multiple options.\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --lr LR\n",
            "    Learning\n",
            "    rate to set\n",
            "    for the\n",
            "    model\n",
            "    (default:\n",
            "    0.001)\n",
            "  --dataset DATASET\n",
            "    The dataset\n",
            "    to use.\n",
            "    (default:\n",
            "    None)\n",
            "  --model MODEL\n",
            "    The model\n",
            "    to use for\n",
            "    training.\n",
            "    (default:\n",
            "    None)\n",
            "  --epochs EPOCHS\n",
            "    Number of\n",
            "    epochs to\n",
            "    run the\n",
            "    training\n",
            "    for.\n",
            "    (default:\n",
            "    1)\n",
            "  --lr_scheduler LR_SCHEDULER\n",
            "    Type of\n",
            "    Learning\n",
            "    Rate\n",
            "    scheduler\n",
            "    to use.\n",
            "    Available\n",
            "    options: 1.\n",
            "    `StepLR` 2.\n",
            "    `OneCycleLR\n",
            "    ` (default:\n",
            "    OneCycleLR)\n",
            "  --gamma GAMMA\n",
            "    The `gamma`\n",
            "    value to\n",
            "    use for the\n",
            "    LR\n",
            "    scheduler.\n",
            "    (default:\n",
            "    0.9)\n",
            "  --step_size STEP_SIZE\n",
            "    Step Size\n",
            "    for the LR\n",
            "    scheduler\n",
            "    to change\n",
            "    LR = LR *\n",
            "    gamma after\n",
            "    the step\n",
            "    size.\n",
            "    (default:\n",
            "    1)\n",
            "  --optim OPTIM\n",
            "    Optimizer\n",
            "    to select\n",
            "    (sgd or\n",
            "    adam)\n",
            "    (default:\n",
            "    sgd)\n",
            "  --save SAVE\n",
            "    To save the\n",
            "    model or\n",
            "    not after\n",
            "    training.\n",
            "    (default:\n",
            "    False)\n",
            "  --max_lr MAX_LR\n",
            "    The maximum\n",
            "    LR to be\n",
            "    used with\n",
            "    the One\n",
            "    Cycle LR\n",
            "    Scheduler.\n",
            "    (default:\n",
            "    10)\n",
            "  --start_lr START_LR\n",
            "    The start\n",
            "    LR to be\n",
            "    used with\n",
            "    the One\n",
            "    Cycle LR\n",
            "    Scheduler\n",
            "    for the\n",
            "    Optimizer.\n",
            "    (default:\n",
            "    0.001)\n",
            "  --batch BATCH\n",
            "    The batch\n",
            "    size for\n",
            "    the\n",
            "    dataloader.\n",
            "    (default:\n",
            "    32)\n",
            "  --pct_start PCT_START\n",
            "    The end of\n",
            "    the warm-up\n",
            "    phase and\n",
            "    the peak or\n",
            "    max LR\n",
            "    epoch as a\n",
            "    float value\n",
            "    out of the\n",
            "    total\n",
            "    epochs.\n",
            "    (default:\n",
            "    0.2)\n",
            "  --cutout_prob CUTOUT_PROB\n",
            "    The\n",
            "    probability\n",
            "    to apply\n",
            "    cutout in\n",
            "    Transforms\n",
            "    part, [0,1]\n",
            "    float.\n",
            "    (default:\n",
            "    0.2)\n",
            "  --anneal_fn ANNEAL_FN\n",
            "    Annealing\n",
            "    function to\n",
            "    use (Linear\n",
            "    or Cosine)\n",
            "    (default:\n",
            "    linear)\n",
            "  --cri CRI\n",
            "    Criterion\n",
            "    to be used\n",
            "    (nll or cro\n",
            "    ssentropy)\n",
            "    (default: c\n",
            "    rossentropy\n",
            "    )\n",
            "  --find_lr FIND_LR\n",
            "    To run LR\n",
            "    Finder\n",
            "    (These are\n",
            "    needed:\n",
            "    model,\n",
            "    criterion,\n",
            "    start_lr,\n",
            "    max_lr, tra\n",
            "    in_loader,\n",
            "    optimizer,\n",
            "    *, optimize\n",
            "    r_type=`ada\n",
            "    m`, weight_\n",
            "    decay=4e-4,\n",
            "    num_iterati\n",
            "    ons=300, lo\n",
            "    g_lr=True, \n",
            "    step_mode=`\n",
            "    exp`)\n",
            "    (default:\n",
            "    False)\n",
            "  --find_lr_iter FIND_LR_ITER\n",
            "    The number\n",
            "    of\n",
            "    iterations\n",
            "    to use in\n",
            "    LR finder.\n",
            "    (default:\n",
            "    200)\n",
            "  --Help\n",
            "    Available\n",
            "    arguments:\n",
            "    1. `--lr`:\n",
            "    Learning\n",
            "    rate,\n",
            "    default\n",
            "    0.001 2. `-\n",
            "    -dataset`:\n",
            "    Selecting\n",
            "    the\n",
            "    dataset,\n",
            "    available\n",
            "    options\n",
            "    MNIST and\n",
            "    CIFAR10 3.\n",
            "    `--model`:\n",
            "    Model name,\n",
            "    check\n",
            "    models.py\n",
            "    4.\n",
            "    `--epochs`:\n",
            "    Setting the\n",
            "    number of\n",
            "    epochs,\n",
            "    default 1.\n",
            "    5. `--lr-\n",
            "    scheduler`:\n",
            "    Selecting\n",
            "    which\n",
            "    learning\n",
            "    rate\n",
            "    scheduler\n",
            "    to use\n",
            "    Available\n",
            "    options: 1.\n",
            "    `steplr` 2.\n",
            "    `cycliclr`\n",
            "    6.\n",
            "    `--gamma`:\n",
            "    Gamma value\n",
            "    to be used\n",
            "    between 0\n",
            "    and 1.0 7. \n",
            "    `--\n",
            "    step_size`:\n",
            "    Number of\n",
            "    steps after\n",
            "    which to\n",
            "    change LR\n",
            "    in the\n",
            "    scheduler\n",
            "    8.\n",
            "    `--optim`:\n",
            "    Type of\n",
            "    optimizer\n",
            "    to use: 1.\n",
            "    SGD 2. Adam\n",
            "    9.\n",
            "    `--save`:\n",
            "    If to save\n",
            "    the model\n",
            "    or not\n",
            "    (true or\n",
            "    false) 10.\n",
            "    `--max-lr`:\n",
            "    If cyclic\n",
            "    policy is\n",
            "    used, the\n",
            "    maximum\n",
            "    learning\n",
            "    rate to be\n",
            "    used. 11.\n",
            "    `--batch`:\n",
            "    The batch\n",
            "    size for\n",
            "    the\n",
            "    dataloader.\n",
            "    12. `--\n",
            "    pct_start`:\n",
            "    The end of\n",
            "    the warm-up\n",
            "    phase and\n",
            "    peak or max\n",
            "    LR epoch as\n",
            "    a float\n",
            "    value out\n",
            "    of total\n",
            "    epochs. 13.\n",
            "    `--\n",
            "    anneal_fn`:\n",
            "    Annealing\n",
            "    function to\n",
            "    decrease LR\n",
            "    in the\n",
            "    cool-down\n",
            "    phase of\n",
            "    the LR\n",
            "    scheduler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --dataset cifar10 --model s10resnet --optim adam --cri crossentropy --find_lr True --start_lr 0.001 --find_lr_iter 400 --max_lr 10 --cutout_prob 0.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gn-94Tom464",
        "outputId": "67dab735-996c-4241-ad6f-0216d00053f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-22 01:39:51.931320: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-22 01:39:53.702674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "All requirements installed successfully.\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:13<00:00, 12807157.88it/s]\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "\n",
            "\n",
            "⏳ Computing mean and standard deviation...\n",
            "\n",
            "Done ✅\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "\n",
            "Running LR finder... 🔍👀 \n",
            "Start LR: 0.001, End LR: 10.0, iterations: 400, step mode: exp\n",
            "\n",
            " 68% 273/400 [00:07<00:02, 56.43it/s]Stopping early, the loss has diverged\n",
            " 68% 274/400 [00:07<00:03, 34.36it/s]\n",
            "Learning rate search finished. See the graph with {finder_name}.plot()\n",
            "LR suggestion: steepest gradient\n",
            "Suggested LR: 5.55E-02\n",
            "Figure(640x480)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --dataset cifar10 --model s10resnet --epochs 24 --lr_scheduler onecycle --max_lr 5.55E-02 --batch 512 --anneal_fn linear --pct_start 4/24 --cutout_prob 0.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx3YS2YunV32",
        "outputId": "2a24c29c-07a0-4d97-a012-017a79e97428"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-22 01:41:07.887103: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-22 01:41:08.979361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "All requirements installed successfully.\n",
            "Files already downloaded and verified\n",
            "\n",
            "\n",
            "⏳ Computing mean and standard deviation...\n",
            "\n",
            "Done ✅\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "torch.Size([512, 3, 32, 32])\n",
            "torch.Size([512])\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Figure(1500x1500)\n",
            "\n",
            "➤ Training started...→\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 1  |  LR: 0.0055500000\n",
            "Loss=1.5180742740631104 Accuracy=30.63: 100% 98/98 [00:32<00:00,  3.06it/s]\n",
            "\n",
            "Test set: Average loss: 0.00316, Accuracy: 4517/10000 (45.170%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 2  |  LR: 0.0180694373\n",
            "Loss=1.2580416202545166 Accuracy=47.86: 100% 98/98 [00:19<00:00,  5.00it/s]\n",
            "\n",
            "Test set: Average loss: 0.00325, Accuracy: 4891/10000 (48.910%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 3  |  LR: 0.0305888747\n",
            "Loss=0.9231976866722107 Accuracy=58.90: 100% 98/98 [00:20<00:00,  4.73it/s]\n",
            "\n",
            "Test set: Average loss: 0.00434, Accuracy: 4724/10000 (47.240%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 4  |  LR: 0.0431083120\n",
            "Loss=1.2457891702651978 Accuracy=67.95: 100% 98/98 [00:20<00:00,  4.87it/s]\n",
            "\n",
            "Test set: Average loss: 0.00384, Accuracy: 5280/10000 (52.800%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 5  |  LR: 0.0554717120\n",
            "Loss=1.0096088647842407 Accuracy=72.64: 100% 98/98 [00:19<00:00,  4.94it/s]\n",
            "\n",
            "Test set: Average loss: 0.00195, Accuracy: 7063/10000 (70.630%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 6  |  LR: 0.0526994870\n",
            "Loss=0.6499779224395752 Accuracy=76.25: 100% 98/98 [00:20<00:00,  4.83it/s]\n",
            "\n",
            "Test set: Average loss: 0.00140, Accuracy: 7677/10000 (76.770%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 7  |  LR: 0.0499272620\n",
            "Loss=0.6459105014801025 Accuracy=80.81: 100% 98/98 [00:20<00:00,  4.82it/s]\n",
            "\n",
            "Test set: Average loss: 0.00158, Accuracy: 7628/10000 (76.280%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 8  |  LR: 0.0471550370\n",
            "Loss=0.4208225607872009 Accuracy=83.31: 100% 98/98 [00:19<00:00,  4.94it/s]\n",
            "\n",
            "Test set: Average loss: 0.00131, Accuracy: 7923/10000 (79.230%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 9  |  LR: 0.0443828120\n",
            "Loss=0.49429306387901306 Accuracy=84.46: 100% 98/98 [00:19<00:00,  4.93it/s]\n",
            "\n",
            "Test set: Average loss: 0.00118, Accuracy: 8116/10000 (81.160%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 10  |  LR: 0.0416105870\n",
            "Loss=0.48184794187545776 Accuracy=86.56: 100% 98/98 [00:20<00:00,  4.87it/s]\n",
            "\n",
            "Test set: Average loss: 0.00097, Accuracy: 8406/10000 (84.060%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 11  |  LR: 0.0388383620\n",
            "Loss=0.3571338653564453 Accuracy=87.75: 100% 98/98 [00:19<00:00,  4.94it/s]\n",
            "\n",
            "Test set: Average loss: 0.00095, Accuracy: 8448/10000 (84.480%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 12  |  LR: 0.0360661370\n",
            "Loss=0.32210755348205566 Accuracy=89.08: 100% 98/98 [00:19<00:00,  4.94it/s]\n",
            "\n",
            "Test set: Average loss: 0.00086, Accuracy: 8630/10000 (86.300%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 13  |  LR: 0.0332939120\n",
            "Loss=0.3231775462627411 Accuracy=89.54: 100% 98/98 [00:20<00:00,  4.87it/s]\n",
            "\n",
            "Test set: Average loss: 0.00085, Accuracy: 8615/10000 (86.150%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 14  |  LR: 0.0305216870\n",
            "Loss=0.3741612136363983 Accuracy=90.33: 100% 98/98 [00:19<00:00,  4.91it/s]\n",
            "\n",
            "Test set: Average loss: 0.00084, Accuracy: 8660/10000 (86.600%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 15  |  LR: 0.0277494620\n",
            "Loss=0.3446144461631775 Accuracy=91.64: 100% 98/98 [00:19<00:00,  4.95it/s]\n",
            "\n",
            "Test set: Average loss: 0.00081, Accuracy: 8721/10000 (87.210%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 16  |  LR: 0.0249772370\n",
            "Loss=0.23858210444450378 Accuracy=92.30: 100% 98/98 [00:20<00:00,  4.69it/s]\n",
            "\n",
            "Test set: Average loss: 0.00076, Accuracy: 8793/10000 (87.930%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 17  |  LR: 0.0222050120\n",
            "Loss=0.17826606333255768 Accuracy=93.00: 100% 98/98 [00:19<00:00,  4.92it/s]\n",
            "\n",
            "Test set: Average loss: 0.00075, Accuracy: 8831/10000 (88.310%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 18  |  LR: 0.0194327870\n",
            "Loss=0.19204750657081604 Accuracy=93.74: 100% 98/98 [00:19<00:00,  4.94it/s]\n",
            "\n",
            "Test set: Average loss: 0.00071, Accuracy: 8916/10000 (89.160%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 19  |  LR: 0.0166605620\n",
            "Loss=0.1812313050031662 Accuracy=94.37: 100% 98/98 [00:20<00:00,  4.89it/s]\n",
            "\n",
            "Test set: Average loss: 0.00071, Accuracy: 8919/10000 (89.190%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 20  |  LR: 0.0138883370\n",
            "Loss=0.15617507696151733 Accuracy=95.30: 100% 98/98 [00:20<00:00,  4.83it/s]\n",
            "\n",
            "Test set: Average loss: 0.00075, Accuracy: 8890/10000 (88.900%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 21  |  LR: 0.0111161120\n",
            "Loss=0.11813001334667206 Accuracy=96.05: 100% 98/98 [00:19<00:00,  4.92it/s]\n",
            "\n",
            "Test set: Average loss: 0.00071, Accuracy: 8960/10000 (89.600%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 22  |  LR: 0.0083438870\n",
            "Loss=0.09132890403270721 Accuracy=96.50: 100% 98/98 [00:20<00:00,  4.78it/s]\n",
            "\n",
            "Test set: Average loss: 0.00069, Accuracy: 8999/10000 (89.990%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 23  |  LR: 0.0055716620\n",
            "Loss=0.08714704215526581 Accuracy=97.18: 100% 98/98 [00:20<00:00,  4.85it/s]\n",
            "\n",
            "Test set: Average loss: 0.00068, Accuracy: 9033/10000 (90.330%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 24  |  LR: 0.0027994370\n",
            "Loss=0.06619299948215485 Accuracy=97.67: 100% 98/98 [00:19<00:00,  4.93it/s]\n",
            "\n",
            "Test set: Average loss: 0.00066, Accuracy: 9044/10000 (90.440%)\n",
            "\n",
            "Figure(1000x500)\n",
            "Figure(1000x500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kudh3DegxY5l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}