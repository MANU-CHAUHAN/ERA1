{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "aMIimxGGLcwa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ERA1"
      ],
      "metadata": {
        "id": "LxPKBAVSogic"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s18qOrYNmklj",
        "outputId": "da928e9b-312b-4ac3-bb8d-2f9629b43c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ERA1'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 128 (delta 66), reused 91 (delta 32), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (128/128), 13.06 MiB | 19.25 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/MANU-CHAUHAN/ERA1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRyVWVWtmxTZ",
        "outputId": "811f4959-c11e-4233-92f8-fb66f244794d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mERA1\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ERA1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jaj2bCWRm08y",
        "outputId": "9872afbf-f01e-4795-894a-bb608abc3c4f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ERA1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmklCZ6um3gE",
        "outputId": "8cb80e0d-ca31-4eca-e185-7a17bba89356"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main.py    README.md         \u001b[0m\u001b[01;34mresources\u001b[0m/  \u001b[01;34mS7\u001b[0m/  test.py   webapp.py\n",
            "models.py  requirements.txt  \u001b[01;34mS10\u001b[0m/        \u001b[01;34mS9\u001b[0m/  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W_yjPzCnGp0",
        "outputId": "4e4f83d5-c5ce-4e5e-a2b5-47a51f4eb312"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-22 01:39:36.059916: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-22 01:39:37.089705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "All requirements installed successfully.\n",
            "usage: main.py\n",
            "       [-h]\n",
            "       [--lr LR]\n",
            "       [--dataset DATASET]\n",
            "       [--model MODEL]\n",
            "       [--epochs EPOCHS]\n",
            "       [--lr_scheduler LR_SCHEDULER]\n",
            "       [--gamma GAMMA]\n",
            "       [--step_size STEP_SIZE]\n",
            "       [--optim OPTIM]\n",
            "       [--save SAVE]\n",
            "       [--max_lr MAX_LR]\n",
            "       [--start_lr START_LR]\n",
            "       [--batch BATCH]\n",
            "       [--pct_start PCT_START]\n",
            "       [--cutout_prob CUTOUT_PROB]\n",
            "       [--anneal_fn ANNEAL_FN]\n",
            "       [--cri CRI]\n",
            "       [--find_lr FIND_LR]\n",
            "       [--find_lr_iter FIND_LR_ITER]\n",
            "       [--Help]\n",
            "\n",
            "Training Deep Learning program for various models with multiple options.\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --lr LR\n",
            "    Learning\n",
            "    rate to set\n",
            "    for the\n",
            "    model\n",
            "    (default:\n",
            "    0.001)\n",
            "  --dataset DATASET\n",
            "    The dataset\n",
            "    to use.\n",
            "    (default:\n",
            "    None)\n",
            "  --model MODEL\n",
            "    The model\n",
            "    to use for\n",
            "    training.\n",
            "    (default:\n",
            "    None)\n",
            "  --epochs EPOCHS\n",
            "    Number of\n",
            "    epochs to\n",
            "    run the\n",
            "    training\n",
            "    for.\n",
            "    (default:\n",
            "    1)\n",
            "  --lr_scheduler LR_SCHEDULER\n",
            "    Type of\n",
            "    Learning\n",
            "    Rate\n",
            "    scheduler\n",
            "    to use.\n",
            "    Available\n",
            "    options: 1.\n",
            "    `StepLR` 2.\n",
            "    `OneCycleLR\n",
            "    ` (default:\n",
            "    OneCycleLR)\n",
            "  --gamma GAMMA\n",
            "    The `gamma`\n",
            "    value to\n",
            "    use for the\n",
            "    LR\n",
            "    scheduler.\n",
            "    (default:\n",
            "    0.9)\n",
            "  --step_size STEP_SIZE\n",
            "    Step Size\n",
            "    for the LR\n",
            "    scheduler\n",
            "    to change\n",
            "    LR = LR *\n",
            "    gamma after\n",
            "    the step\n",
            "    size.\n",
            "    (default:\n",
            "    1)\n",
            "  --optim OPTIM\n",
            "    Optimizer\n",
            "    to select\n",
            "    (sgd or\n",
            "    adam)\n",
            "    (default:\n",
            "    sgd)\n",
            "  --save SAVE\n",
            "    To save the\n",
            "    model or\n",
            "    not after\n",
            "    training.\n",
            "    (default:\n",
            "    False)\n",
            "  --max_lr MAX_LR\n",
            "    The maximum\n",
            "    LR to be\n",
            "    used with\n",
            "    the One\n",
            "    Cycle LR\n",
            "    Scheduler.\n",
            "    (default:\n",
            "    10)\n",
            "  --start_lr START_LR\n",
            "    The start\n",
            "    LR to be\n",
            "    used with\n",
            "    the One\n",
            "    Cycle LR\n",
            "    Scheduler\n",
            "    for the\n",
            "    Optimizer.\n",
            "    (default:\n",
            "    0.001)\n",
            "  --batch BATCH\n",
            "    The batch\n",
            "    size for\n",
            "    the\n",
            "    dataloader.\n",
            "    (default:\n",
            "    32)\n",
            "  --pct_start PCT_START\n",
            "    The end of\n",
            "    the warm-up\n",
            "    phase and\n",
            "    the peak or\n",
            "    max LR\n",
            "    epoch as a\n",
            "    float value\n",
            "    out of the\n",
            "    total\n",
            "    epochs.\n",
            "    (default:\n",
            "    0.2)\n",
            "  --cutout_prob CUTOUT_PROB\n",
            "    The\n",
            "    probability\n",
            "    to apply\n",
            "    cutout in\n",
            "    Transforms\n",
            "    part, [0,1]\n",
            "    float.\n",
            "    (default:\n",
            "    0.2)\n",
            "  --anneal_fn ANNEAL_FN\n",
            "    Annealing\n",
            "    function to\n",
            "    use (Linear\n",
            "    or Cosine)\n",
            "    (default:\n",
            "    linear)\n",
            "  --cri CRI\n",
            "    Criterion\n",
            "    to be used\n",
            "    (nll or cro\n",
            "    ssentropy)\n",
            "    (default: c\n",
            "    rossentropy\n",
            "    )\n",
            "  --find_lr FIND_LR\n",
            "    To run LR\n",
            "    Finder\n",
            "    (These are\n",
            "    needed:\n",
            "    model,\n",
            "    criterion,\n",
            "    start_lr,\n",
            "    max_lr, tra\n",
            "    in_loader,\n",
            "    optimizer,\n",
            "    *, optimize\n",
            "    r_type=`ada\n",
            "    m`, weight_\n",
            "    decay=4e-4,\n",
            "    num_iterati\n",
            "    ons=300, lo\n",
            "    g_lr=True, \n",
            "    step_mode=`\n",
            "    exp`)\n",
            "    (default:\n",
            "    False)\n",
            "  --find_lr_iter FIND_LR_ITER\n",
            "    The number\n",
            "    of\n",
            "    iterations\n",
            "    to use in\n",
            "    LR finder.\n",
            "    (default:\n",
            "    200)\n",
            "  --Help\n",
            "    Available\n",
            "    arguments:\n",
            "    1. `--lr`:\n",
            "    Learning\n",
            "    rate,\n",
            "    default\n",
            "    0.001 2. `-\n",
            "    -dataset`:\n",
            "    Selecting\n",
            "    the\n",
            "    dataset,\n",
            "    available\n",
            "    options\n",
            "    MNIST and\n",
            "    CIFAR10 3.\n",
            "    `--model`:\n",
            "    Model name,\n",
            "    check\n",
            "    models.py\n",
            "    4.\n",
            "    `--epochs`:\n",
            "    Setting the\n",
            "    number of\n",
            "    epochs,\n",
            "    default 1.\n",
            "    5. `--lr-\n",
            "    scheduler`:\n",
            "    Selecting\n",
            "    which\n",
            "    learning\n",
            "    rate\n",
            "    scheduler\n",
            "    to use\n",
            "    Available\n",
            "    options: 1.\n",
            "    `steplr` 2.\n",
            "    `cycliclr`\n",
            "    6.\n",
            "    `--gamma`:\n",
            "    Gamma value\n",
            "    to be used\n",
            "    between 0\n",
            "    and 1.0 7. \n",
            "    `--\n",
            "    step_size`:\n",
            "    Number of\n",
            "    steps after\n",
            "    which to\n",
            "    change LR\n",
            "    in the\n",
            "    scheduler\n",
            "    8.\n",
            "    `--optim`:\n",
            "    Type of\n",
            "    optimizer\n",
            "    to use: 1.\n",
            "    SGD 2. Adam\n",
            "    9.\n",
            "    `--save`:\n",
            "    If to save\n",
            "    the model\n",
            "    or not\n",
            "    (true or\n",
            "    false) 10.\n",
            "    `--max-lr`:\n",
            "    If cyclic\n",
            "    policy is\n",
            "    used, the\n",
            "    maximum\n",
            "    learning\n",
            "    rate to be\n",
            "    used. 11.\n",
            "    `--batch`:\n",
            "    The batch\n",
            "    size for\n",
            "    the\n",
            "    dataloader.\n",
            "    12. `--\n",
            "    pct_start`:\n",
            "    The end of\n",
            "    the warm-up\n",
            "    phase and\n",
            "    peak or max\n",
            "    LR epoch as\n",
            "    a float\n",
            "    value out\n",
            "    of total\n",
            "    epochs. 13.\n",
            "    `--\n",
            "    anneal_fn`:\n",
            "    Annealing\n",
            "    function to\n",
            "    decrease LR\n",
            "    in the\n",
            "    cool-down\n",
            "    phase of\n",
            "    the LR\n",
            "    scheduler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --dataset cifar10 --model s10resnet --optim adam --cri crossentropy --find_lr True --start_lr 0.001 --find_lr_iter 400 --max_lr 10 --cutout_prob 0.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gn-94Tom464",
        "outputId": "8bf2628d-d9cc-4ca9-eaf4-a06f32fbaa19"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-22 01:50:58.632807: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-22 01:50:59.647437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "All requirements installed successfully.\n",
            "Files already downloaded and verified\n",
            "\n",
            "\n",
            "‚è≥ Computing mean and standard deviation...\n",
            "\n",
            "Done ‚úÖ\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "\n",
            "Running LR finder... üîçüëÄ \n",
            "Start LR: 0.001, End LR: 10.0, iterations: 400, step mode: exp\n",
            "\n",
            " 69% 276/400 [00:08<00:02, 54.86it/s]Stopping early, the loss has diverged\n",
            " 69% 276/400 [00:08<00:03, 34.10it/s]\n",
            "Learning rate search finished. See the graph with {finder_name}.plot()\n",
            "LR suggestion: steepest gradient\n",
            "Suggested LR: 3.26E-02\n",
            "Figure(640x480)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --dataset cifar10 --model s10resnet --epochs 24 --lr_scheduler onecycle --max_lr 3.26E-02 --batch 512 --anneal_fn linear --pct_start 4/24 --cutout_prob 0.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx3YS2YunV32",
        "outputId": "09904170-7752-47c4-c245-746519e73761"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-22 02:04:34.751981: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-22 02:04:35.924712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "All requirements installed successfully.\n",
            "Files already downloaded and verified\n",
            "\n",
            "\n",
            "‚è≥ Computing mean and standard deviation...\n",
            "\n",
            "Done ‚úÖ\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "torch.Size([512, 3, 32, 32])\n",
            "torch.Size([512])\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Figure(1500x1500)\n",
            "\n",
            "‚û§ Training started...‚Üí\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 1  |  LR: 0.0032600000\n",
            "Loss=1.3835639953613281 Accuracy=42.09: 100% 98/98 [00:32<00:00,  3.05it/s]\n",
            "\n",
            "Test set: Average loss: 0.00268, Accuracy: 5414/10000 (54.140%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 2  |  LR: 0.0106137596\n",
            "Loss=0.9546510577201843 Accuracy=60.41: 100% 98/98 [00:19<00:00,  4.93it/s]\n",
            "\n",
            "Test set: Average loss: 0.00244, Accuracy: 6331/10000 (63.310%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 3  |  LR: 0.0179675192\n",
            "Loss=0.9061620831489563 Accuracy=65.92: 100% 98/98 [00:20<00:00,  4.73it/s]\n",
            "\n",
            "Test set: Average loss: 0.00208, Accuracy: 6880/10000 (68.800%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 4  |  LR: 0.0253212788\n",
            "Loss=0.6151957511901855 Accuracy=73.27: 100% 98/98 [00:20<00:00,  4.80it/s]\n",
            "\n",
            "Test set: Average loss: 0.00148, Accuracy: 7563/10000 (75.630%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 5  |  LR: 0.0325833840\n",
            "Loss=0.5857670903205872 Accuracy=79.06: 100% 98/98 [00:21<00:00,  4.61it/s]\n",
            "\n",
            "Test set: Average loss: 0.00174, Accuracy: 7529/10000 (75.290%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 6  |  LR: 0.0309550140\n",
            "Loss=0.5652981400489807 Accuracy=81.64: 100% 98/98 [00:20<00:00,  4.72it/s]\n",
            "\n",
            "Test set: Average loss: 0.00120, Accuracy: 8154/10000 (81.540%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 7  |  LR: 0.0293266440\n",
            "Loss=0.43527093529701233 Accuracy=82.55: 100% 98/98 [00:20<00:00,  4.87it/s]\n",
            "\n",
            "Test set: Average loss: 0.00108, Accuracy: 8286/10000 (82.860%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 8  |  LR: 0.0276982740\n",
            "Loss=0.3458488881587982 Accuracy=85.40: 100% 98/98 [00:20<00:00,  4.80it/s]\n",
            "\n",
            "Test set: Average loss: 0.00093, Accuracy: 8469/10000 (84.690%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 9  |  LR: 0.0260699040\n",
            "Loss=0.34569698572158813 Accuracy=87.18: 100% 98/98 [00:21<00:00,  4.51it/s]\n",
            "\n",
            "Test set: Average loss: 0.00084, Accuracy: 8620/10000 (86.200%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 10  |  LR: 0.0244415340\n",
            "Loss=0.36366939544677734 Accuracy=88.08: 100% 98/98 [00:20<00:00,  4.69it/s]\n",
            "\n",
            "Test set: Average loss: 0.00090, Accuracy: 8589/10000 (85.890%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 11  |  LR: 0.0228131640\n",
            "Loss=0.3255164921283722 Accuracy=89.09: 100% 98/98 [00:20<00:00,  4.84it/s]\n",
            "\n",
            "Test set: Average loss: 0.00116, Accuracy: 8283/10000 (82.830%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 12  |  LR: 0.0211847940\n",
            "Loss=0.34058526158332825 Accuracy=90.07: 100% 98/98 [00:20<00:00,  4.87it/s]\n",
            "\n",
            "Test set: Average loss: 0.00085, Accuracy: 8696/10000 (86.960%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 13  |  LR: 0.0195564240\n",
            "Loss=0.18036054074764252 Accuracy=91.10: 100% 98/98 [00:20<00:00,  4.68it/s]\n",
            "\n",
            "Test set: Average loss: 0.00083, Accuracy: 8757/10000 (87.570%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 14  |  LR: 0.0179280540\n",
            "Loss=0.21329429745674133 Accuracy=91.87: 100% 98/98 [00:21<00:00,  4.50it/s]\n",
            "\n",
            "Test set: Average loss: 0.00082, Accuracy: 8734/10000 (87.340%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 15  |  LR: 0.0162996840\n",
            "Loss=0.3170209527015686 Accuracy=92.57: 100% 98/98 [00:20<00:00,  4.85it/s]\n",
            "\n",
            "Test set: Average loss: 0.00073, Accuracy: 8872/10000 (88.720%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 16  |  LR: 0.0146713140\n",
            "Loss=0.1904146373271942 Accuracy=93.35: 100% 98/98 [00:21<00:00,  4.66it/s]\n",
            "\n",
            "Test set: Average loss: 0.00084, Accuracy: 8776/10000 (87.760%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 17  |  LR: 0.0130429440\n",
            "Loss=0.15104053914546967 Accuracy=93.59: 100% 98/98 [00:20<00:00,  4.70it/s]\n",
            "\n",
            "Test set: Average loss: 0.00074, Accuracy: 8887/10000 (88.870%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 18  |  LR: 0.0114145740\n",
            "Loss=0.18315741419792175 Accuracy=94.18: 100% 98/98 [00:20<00:00,  4.80it/s]\n",
            "\n",
            "Test set: Average loss: 0.00074, Accuracy: 8884/10000 (88.840%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 19  |  LR: 0.0097862040\n",
            "Loss=0.1209585890173912 Accuracy=95.38: 100% 98/98 [00:20<00:00,  4.84it/s]\n",
            "\n",
            "Test set: Average loss: 0.00065, Accuracy: 9037/10000 (90.370%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 20  |  LR: 0.0081578340\n",
            "Loss=0.09489157795906067 Accuracy=95.75: 100% 98/98 [00:21<00:00,  4.57it/s]\n",
            "\n",
            "Test set: Average loss: 0.00067, Accuracy: 9022/10000 (90.220%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 21  |  LR: 0.0065294640\n",
            "Loss=0.12329799681901932 Accuracy=96.27: 100% 98/98 [00:20<00:00,  4.70it/s]\n",
            "\n",
            "Test set: Average loss: 0.00065, Accuracy: 9035/10000 (90.350%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 22  |  LR: 0.0049010940\n",
            "Loss=0.07098864018917084 Accuracy=96.83: 100% 98/98 [00:20<00:00,  4.83it/s]\n",
            "\n",
            "Test set: Average loss: 0.00062, Accuracy: 9065/10000 (90.650%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 23  |  LR: 0.0032727240\n",
            "Loss=0.06662867963314056 Accuracy=97.20: 100% 98/98 [00:20<00:00,  4.85it/s]\n",
            "\n",
            "Test set: Average loss: 0.00060, Accuracy: 9100/10000 (91.000%)\n",
            "\n",
            "  0% 0/98 [00:00<?, ?it/s]\n",
            "Epoch num: 24  |  LR: 0.0016443540\n",
            "Loss=0.06973700225353241 Accuracy=97.72: 100% 98/98 [00:20<00:00,  4.70it/s]\n",
            "\n",
            "Test set: Average loss: 0.00060, Accuracy: 9141/10000 (91.410%)\n",
            "\n",
            "Figure(1000x500)\n",
            "Figure(1000x500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kudh3DegxY5l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}